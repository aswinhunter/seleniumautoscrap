# This is the name of the workflow
name: Daily Price Scraper

# Controls when the action will run.
on:
  # This allows you to run the workflow manually from the Actions tab
  workflow_dispatch:
  
  # This sets the schedule. The format is CRON.
  # '0 2 * * *' means "run at 2:00 AM UTC every day".
  # You can change the time. Use a CRON generator online to help.
  schedule:
    - cron: '0 2 * * *'

# A workflow run is made up of one or more jobs that can run sequentially or in parallel
jobs:
  scrape:
    # The type of runner that the job will run on
    runs-on: ubuntu-latest

    # Steps represent a sequence of tasks that will be executed as part of the job
    steps:
      # Step 1: Checks-out your repository under $GITHUB_WORKSPACE, so your job can access it
      - name: Check out repository
        uses: actions/checkout@v4

      # Step 2: Sets up a Python environment for you to use
      - name: Set up Python
        uses: actions/setup-python@v5
        with:
          python-version: '3.10' # You can use other versions like 3.9, 3.11 etc.

      # Step 3: Installs the libraries from your requirements.txt file
      - name: Install dependencies
        run: |
          python -m pip install --upgrade pip
          pip install -r requirements.txt
          # Install system dependencies for Selenium/Chrome
          sudo apt-get update
          sudo apt-get install -y google-chrome-stable

      # Step 4: VERY IMPORTANT - Create the service account key from the GitHub Secret
      - name: Create Firebase Service Account file
        run: echo "${{ secrets.FIREBASE_CREDENTIALS }}" > serviceAccountKey.json
        # This takes the secret you saved in GitHub settings and creates the actual file
        # that your Python script needs to connect to Firebase.

      # Step 5: Run your Python script
      - name: Run the scraper script
        run: python scraper.py
